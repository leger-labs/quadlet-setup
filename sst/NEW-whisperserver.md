Looking at your existing setup, you already have `containers/systemd/sst/sst.container` using `faster-whisper-server:latest-cpu`. This is actually a good architecture! However, let's review the philosophy and improve it:

## Philosophy: Host vs Container for Whisper

### My Recommendation: Hybrid (like RamaLama approach)

**Host-based whisper.cpp** + **Containerized API server**

**Reasoning:**
1. ✅ **Models on host**: Store Whisper models in `~/.cache/whisper.cpp/` (like RamaLama models)
2. ✅ **Binary on host**: Install whisper.cpp natively for "close to metal"
3. ✅ **Service containerized**: Wrap in container for consistency with stack
4. ✅ **Vulkan support**: whisper.cpp supports Vulkan for AMD GPU

This is **better than** just running faster-whisper-server container because:
- Matches your RamaLama philosophy
- Native performance via whisper.cpp
- Same model cache whether running in container or CLI

---

## Complete Implementation

### 1. Install whisper.cpp on Host from dnf
### 2. Whisper Server Container (Quadlet)

**Folder:** `dot_config/containers/systemd/whisper/`

#### `whisper.container.tmpl`

```ini
# Generated by Chezmoi - DO NOT EDIT MANUALLY
# Whisper STT - Speech-to-Text service

[Unit]
Description=Whisper STT - Speech-to-Text Server
After=network-online.target llm.network.service
Requires=llm.network.service

[Container]
Image=docker.io/fedirz/faster-whisper-server:latest-cpu
AutoUpdate=registry
ContainerName=whisper

# Network - internal + published for OpenWebUI
Network=llm.network
PublishPort=127.0.0.1:{{ .published_ports.whisper }}:{{ .ports.whisper }}

# Mount whisper model cache from host
Volume={{ .chezmoi.homeDir }}/.cache/whisper.cpp:/root/.cache/whisper:Z

# GPU access for AMD/Vulkan
AddDevice=/dev/dri
AddDevice=/dev/kfd
PodmanArgs=--security-opt=label=disable

# Environment
Environment=WHISPER_MODEL={{ .services.whisper.model }}
Environment=WHISPER_LANGUAGE=en
Environment=WHISPER_BEAM_SIZE=5

# Health check
HealthCmd="curl -f http://localhost:{{ .ports.whisper }}/health || exit 1"
HealthInterval=30s
HealthTimeout=5s
HealthRetries=3
HealthStartPeriod=30s

[Service]
Slice=llm.slice
TimeoutStartSec=900
Restart=on-failure
RestartSec=10

[Install]
WantedBy=scroll-session.target
```

#### `whisper.volume`

```ini
[Volume]
VolumeName=whisper-cache
```

#### `README.md`

```markdown
# Whisper STT - Speech-to-Text Service

## Overview

Whisper provides OpenAI-compatible speech-to-text transcription using Whisper models with Vulkan GPU acceleration for AMD hardware.

## Architecture

```
Request Flow:
User → OpenWebUI → whisper:8000 → faster-whisper-server → Whisper models

Container Communication:
whisper (container)
    ├─ Mounted: ~/.cache/whisper.cpp (model cache)
    ├─ GPU: /dev/dri, /dev/kfd (Vulkan)
    └─ Network: llm.network (internal communication)
```

## Configuration

Models are cached in `~/.cache/whisper.cpp/` on the host and mounted into the container.

### Available Models

| Model | Size | Language | Use Case |
|-------|------|----------|----------|
| `tiny` | 75 MB | English | Fast, low accuracy |
| `base` | 142 MB | English | Good balance |
| `small` | 466 MB | Multi | Better accuracy |
| `medium` | 1.5 GB | Multi | High accuracy |
| `large-v2` | 2.9 GB | Multi | Best accuracy |

Default: `base` (configured in `.chezmoi.yaml.tmpl`)

## Service Control

```bash
# Start whisper
systemctl --user start whisper

# Status
systemctl --user status whisper

# Logs
journalctl --user -u whisper -f

# Enable on boot
systemctl --user enable whisper
```

## Model Management

### Using Host Binary

```bash
# Download specific model
cd ~/.cache/whisper.cpp
curl -L "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin" -o ggml-base.en.bin

# Test transcription with host binary
whisper-cpp -m ~/.cache/whisper.cpp/ggml-base.en.bin -f audio.wav
```

### Via Container

Models are automatically downloaded on first use by the container.

## API Endpoints

### OpenAI-compatible

- `POST /v1/audio/transcriptions` - Transcribe audio to text
  ```bash
  curl -X POST http://localhost:8000/v1/audio/transcriptions \
    -H "Content-Type: multipart/form-data" \
    -F "file=@audio.mp3" \
    -F "model=whisper-1"
  ```

- `POST /v1/audio/translations` - Translate audio to English
  ```bash
  curl -X POST http://localhost:8000/v1/audio/translations \
    -H "Content-Type: multipart/form-data" \
    -F "file=@audio.mp3" \
    -F "model=whisper-1"
  ```

### Health Check

- `GET /health` - Service health status

## OpenWebUI Integration

Configured in `openwebui.env.tmpl`:

```bash
AUDIO_STT_ENGINE=openai
AUDIO_STT_OPENAI_API_BASE_URL=http://whisper:8000/v1
AUDIO_STT_OPENAI_API_KEY=sk-whisper  # Any value works for local
```

## Testing

```bash
# Test via curl
curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -H "Content-Type: multipart/form-data" \
  -F "file=@test.mp3" \
  -F "model=whisper-1" \
  -F "language=en"

# Test via OpenWebUI
# Click microphone icon in chat
# Record or upload audio
# Should transcribe automatically
```

## Performance

| Device | Model | RTF | Notes |
|--------|-------|-----|-------|
| AMD Vulkan | base | ~5x | Real-time capable |
| AMD Vulkan | small | ~3x | Better accuracy |
| AMD Vulkan | medium | ~1.5x | Near real-time |

**RTF** = Real-Time Factor (5x = 5 seconds of processing per 1 second of audio)

## GPU Access

The container has access to AMD GPU via:
- `/dev/dri` - Direct Rendering Infrastructure
- `/dev/kfd` - Kernel Fusion Driver
- Vulkan backend in whisper.cpp

## Troubleshooting

### Service won't start
```bash
# Check logs
journalctl --user -u whisper -f

# Verify GPU access
podman exec whisper ls -la /dev/dri /dev/kfd

# Test container manually
podman run -it --rm --device=/dev/dri --device=/dev/kfd \
  docker.io/fedirz/faster-whisper-server:latest-cpu
```

### Transcription slow
- Use smaller model (tiny/base)
- Check GPU is being used: `podman exec whisper nvidia-smi` won't work (AMD)
- Check Vulkan: `podman exec whisper vulkaninfo` (if available)

### Model not found
```bash
# List cached models
ls -lh ~/.cache/whisper.cpp/

# Download manually
cd ~/.cache/whisper.cpp
curl -L "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin" -o ggml-base.en.bin
```

### OpenWebUI can't connect
- Verify service running: `systemctl --user status whisper`
- Check network: `podman exec openwebui curl http://whisper:8000/health`
- Verify config in OpenWebUI settings

## Advanced Configuration

### Custom Model Path

Edit `.chezmoi.yaml.tmpl`:
```yaml
services:
  whisper:
    model: "medium"  # Change to tiny/base/small/medium/large-v2
```

Then:
```bash
chezmoi apply
systemctl --user restart whisper
```

### Language-Specific

Add to container environment:
```ini
Environment=WHISPER_LANGUAGE=fr  # French
Environment=WHISPER_LANGUAGE=es  # Spanish
```

### Increase Accuracy

```ini
Environment=WHISPER_BEAM_SIZE=10  # Default: 5
```

Higher beam size = slower but more accurate.
```

---

### 3. Update `.chezmoi.yaml.tmpl`

Add whisper configuration:

```yaml
data:
  # ... existing config ...
  
  # Ports
  ports:
    # ... existing ports ...
    whisper: 8000
  
  # Published ports
  published_ports:
    # ... existing ports ...
    whisper: 8000
  
  # URLs
  urls:
    # ... existing urls ...
    whisper: "https://whisper.{{ .tailscale.hostname }}.{{ .tailscale.tailnet }}"
  
  # Service configuration
  services:
    # ... existing services ...
    whisper:
      model: "base"  # Options: tiny, base, small, medium, large-v2
      language: "en"
      beam_size: 5
```

---

### 4. Update OpenWebUI Configuration

**File:** `containers/systemd/openwebui/openwebui.env.tmpl`

Add STT configuration:

```bash
# ... existing config ...

# ============================================================================
# Speech-to-Text (STT) - Whisper
# ============================================================================
AUDIO_STT_ENGINE=openai
AUDIO_STT_OPENAI_API_BASE_URL=http://whisper:{{ .ports.whisper }}/v1
AUDIO_STT_OPENAI_API_KEY=sk-whisper  # Any value works for local
AUDIO_STT_MODEL=whisper-1

# STT Features
ENABLE_AUDIO_STT=true
AUDIO_STT_SPLIT_ON=punctuation  # or silence
```

Update dependencies in `openwebui.container.tmpl`:

```ini
[Unit]
Description=Open WebUI - AI Chat Interface
After=network-online.target llm.network.service
After=litellm.service openwebui-postgres.service openwebui-redis.service searxng.service docling.service jupyter.service qdrant.service whisper.service
Requires=llm.network.service
Wants=litellm.service openwebui-postgres.service openwebui-redis.service searxng.service docling.service jupyter.service qdrant.service whisper.service
```

---

### 5. Optional: Caddy Route for External Access

**File:** `private_dot_config/caddy/Caddyfile.tmpl`

```caddyfile
# ... existing routes ...

# Whisper STT (optional - usually only internal)
whisper.{{ .tailscale.hostname }}.{{ .tailscale.tailnet }}:443 {
    reverse_proxy localhost:{{ .published_ports.whisper }}
}
```

---

### 6. Desktop Launcher (Optional)

**File:** `private_dot_local/private_share/private_applications/whisper.desktop.tmpl`

```desktop
[Desktop Entry]
Version=1.0
Type=Application
Name=Whisper STT
Comment=Speech-to-Text Service
Exec=google-chrome-stable --app={{ .urls.whisper }}
Icon=microphone
Terminal=false
Categories=Development;AI;Audio;
```

---

## File Structure Summary

```
~/.local/share/chezmoi/
│
├── run_once_before/
│   └── 01-install-whisper.sh.tmpl        ⭐ Install whisper.cpp on host
│
├── private_dot_config/
│   │
│   ├── caddy/
│   │   └── Caddyfile.tmpl                (updated with whisper route)
│   │
│   └── containers/systemd/
│       │
│       ├── whisper/                      ⭐ NEW SERVICE
│       │   ├── whisper.container.tmpl
│       │   ├── whisper.volume
│       │   └── README.md
│       │
│       └── openwebui/
│           └── openwebui.env.tmpl        (updated with STT config)
│           └── openwebui.container.tmpl  (updated dependencies)
```

---

## Complete Setup Flow

### 1. Initial Setup

```bash
# Apply chezmoi (installs whisper.cpp, creates configs)
chezmoi apply -v

# Verify whisper.cpp installed
whisper-cpp --help
whisper-server --help

# Check model cache
ls -lh ~/.cache/whisper.cpp/
```

### 2. Start Services

```bash
# Reload systemd
systemctl --user daemon-reload

# Start whisper
systemctl --user start whisper

# Check status
systemctl --user status whisper

# View logs
journalctl --user -u whisper -f

# Enable on boot
systemctl --user enable whisper
```

### 3. Test Whisper

```bash
# Test API directly
curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -H "Content-Type: multipart/form-data" \
  -F "file=@test.mp3" \
  -F "model=whisper-1"

# Test from OpenWebUI container
podman exec openwebui curl http://whisper:8000/health
```

### 4. Use in OpenWebUI

1. Open OpenWebUI: `https://ai.llm-server.your-tailnet.ts.net`
2. Click microphone icon in chat input
3. Record audio or upload file
4. Whisper automatically transcribes
5. Transcription appears in chat input

---

## Benefits of This Architecture

1. ✅ **Native whisper.cpp**: Installed on host for "close to metal" performance
2. ✅ **Containerized service**: Consistent with rest of stack
3. ✅ **Shared model cache**: Host and container use same models
4. ✅ **Vulkan GPU**: AMD acceleration for faster transcription
5. ✅ **OpenAI-compatible**: Works with any OpenAI STT client
6. ✅ **Integrated with OpenWebUI**: Voice input works out of the box

---

## Performance Expectations

On AMD GPU with Vulkan:

| Model | Size | Speed | Quality | Use Case |
|-------|------|-------|---------|----------|
| **tiny** | 75 MB | 10x realtime | Basic | Quick notes |
| **base** | 142 MB | 5x realtime | Good | General use ✅ |
| **small** | 466 MB | 3x realtime | Better | Important transcripts |
| **medium** | 1.5 GB | 1.5x realtime | Great | High accuracy needed |
| **large-v2** | 2.9 GB | 0.8x realtime | Best | Professional use |

**Recommended**: Start with `base` model for best balance.

This gives you production-ready speech-to-text fully integrated with your LLM stack! 🎤
