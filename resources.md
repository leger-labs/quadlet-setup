Previous output of claude llm scoping, resulting spec below

# Organized Container Structure + Network Architecture

## Part 1: Proper Folder Structure

### The New Organization

```
~/.local/share/chezmoi/
â”‚
â”œâ”€â”€ .chezmoi.yaml.tmpl                    â­ MASTER CONFIG
â”œâ”€â”€ encrypted_private_secrets.yaml        ğŸ”’ ALL SECRETS
â”‚
â”œâ”€â”€ private_dot_local/
â”‚   â””â”€â”€ private_share/
â”‚       â””â”€â”€ private_applications/         â†’ ~/.local/share/applications/
â”‚           â”œâ”€â”€ openwebui.desktop.tmpl    (URL points to {{ .urls.openwebui }})
â”‚           â”œâ”€â”€ litellm.desktop.tmpl      (URL points to {{ .urls.litellm }})
â”‚           â”œâ”€â”€ cockpit.desktop.tmpl
â”‚           â”œâ”€â”€ searxng.desktop.tmpl
â”‚           â””â”€â”€ nextcloud.desktop.tmpl
â”‚
â”œâ”€â”€ private_dot_config/
â”‚   â”‚
â”‚   â”œâ”€â”€ caddy/
â”‚   â”‚   â””â”€â”€ Caddyfile.tmpl                â­ REVERSE PROXY
â”‚   â”‚
â”‚   â”œâ”€â”€ cockpit/
â”‚   â”‚   â””â”€â”€ cockpit.conf.tmpl
â”‚   â”‚
â”‚   â”œâ”€â”€ sway/
â”‚   â”‚   â””â”€â”€ config.tmpl                   (keybinds reference {{ .urls.* }})
â”‚   â”‚
â”‚   â””â”€â”€ containers/
â”‚       â”‚
â”‚       â””â”€â”€ systemd/                      â†’ ~/.config/containers/systemd/
â”‚           â”‚
â”‚           â”œâ”€â”€â”€ ğŸ“ SHARED INFRASTRUCTURE (top level) â”€â”€â”€
â”‚           â”‚
â”‚           â”œâ”€â”€ llm.network               â­ SHARED NETWORK (static)
â”‚           â”‚   â””â”€ All containers connect to this
â”‚           â”‚
â”‚           â”œâ”€â”€ llm.env.tmpl              (optional: shared env vars)
â”‚           â”‚
â”‚           â”œâ”€â”€â”€ ğŸ“ SERVICE FOLDERS (one per service) â”€â”€â”€
â”‚           â”‚
â”‚           â”œâ”€â”€ openwebui/                â­ OPENWEBUI SERVICE
â”‚           â”‚   â”œâ”€â”€ openwebui.container.tmpl
â”‚           â”‚   â”œâ”€â”€ openwebui.volume
â”‚           â”‚   â””â”€â”€ openwebui.env.tmpl    (service-specific config)
â”‚           â”‚
â”‚           â”œâ”€â”€ litellm/                  â­ LITELLM SERVICE
â”‚           â”‚   â”œâ”€â”€ litellm.container.tmpl
â”‚           â”‚   â””â”€â”€ litellm.yaml.tmpl     (service-specific config)
â”‚           â”‚
â”‚           â”œâ”€â”€ llm-postgres/             â­ POSTGRES DATABASE
â”‚           â”‚   â”œâ”€â”€ llm-postgres.container.tmpl
â”‚           â”‚   â””â”€â”€ llm-postgres.volume
â”‚           â”‚
â”‚           â”œâ”€â”€ llm-redis/                â­ REDIS CACHE
â”‚           â”‚   â”œâ”€â”€ llm-redis.container.tmpl
â”‚           â”‚   â””â”€â”€ llm-redis.volume
â”‚           â”‚
â”‚           â”œâ”€â”€ searxng/                  â­ SEARCH ENGINE
â”‚           â”‚   â”œâ”€â”€ searxng.container.tmpl
â”‚           â”‚   â”œâ”€â”€ searxng.volume
â”‚           â”‚   â””â”€â”€ searxng-settings.yml.tmpl
â”‚           â”‚
â”‚           â”œâ”€â”€ docling/                  â­ DOCUMENT PROCESSOR
â”‚           â”‚   â””â”€â”€ docling.container.tmpl
â”‚           â”‚
â”‚           â”œâ”€â”€ tika/                     â­ CONTENT EXTRACTION
â”‚           â”‚   â””â”€â”€ tika.container.tmpl
â”‚           â”‚
â”‚           â”œâ”€â”€ mcp/                      â­ TOOL SERVER
â”‚           â”‚   â”œâ”€â”€ mcp.container.tmpl
â”‚           â”‚   â””â”€â”€ mcp-config.json.tmpl
â”‚           â”‚
â”‚           â””â”€â”€ user/                     â†’ ~/.config/containers/systemd/user/
â”‚               â”œâ”€â”€ nextcloud/
â”‚               â”‚   â””â”€â”€ nextcloud.container.tmpl
â”‚               â””â”€â”€ whisper/
â”‚                   â””â”€â”€ whisper-npu.container.tmpl
â”‚
â””â”€â”€ run_once_after/
    â”œâ”€â”€ 01-setup-tailscale.sh.tmpl
    â”œâ”€â”€ 02-setup-cockpit.sh.tmpl
    â””â”€â”€ 03-reload-systemd.sh.tmpl
```

### Key Changes:

1. âœ… **Each service gets its own folder**
   - Makes organization clear
   - Easy to find related files
   - Can add README.md per service

2. âœ… **Desktop files are templates** (not auto-generated)
   - One .desktop.tmpl per webapp
   - References {{ .urls.* }} from master config
   - Simple and maintainable

3. âœ… **Shared infrastructure at top level**
   - `llm.network` - network all containers use
   - `llm.env.tmpl` - optional shared env vars

---

## Part 2: Network Architecture Explained

### Three Network Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 1: External Access (Tailscale)                        â”‚
â”‚ Purpose: Secure remote access to your services              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  User's Device (anywhere on Tailnet)                        â”‚
â”‚         â”‚                                                    â”‚
â”‚         â”‚ HTTPS over Tailscale VPN                          â”‚
â”‚         â†“                                                    â”‚
â”‚  https://ai.llm-server.your-tailnet.ts.net                 â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 2: Host Proxy (Caddy)                                 â”‚
â”‚ Purpose: Route external requests to local services          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Caddy (port 443)                                           â”‚
â”‚    â”œâ”€ ai.hostname.tailnet â†’ localhost:3000                 â”‚
â”‚    â”œâ”€ litellm.hostname.tailnet â†’ localhost:4000            â”‚
â”‚    â””â”€ search.hostname.tailnet â†’ localhost:8888             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ localhost published ports
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 3: Container Network (Podman)                         â”‚
â”‚ Purpose: Inter-container communication                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  llm.network (10.89.0.0/24)                                 â”‚
â”‚    â”‚                                                         â”‚
â”‚    â”œâ”€ openwebui:8080 â”€â”€â”¬â”€â†’ litellm:4000                    â”‚
â”‚    â”‚                    â”œâ”€â†’ postgres:5432                   â”‚
â”‚    â”‚                    â”œâ”€â†’ redis:6379                      â”‚
â”‚    â”‚                    â”œâ”€â†’ searxng:8080                    â”‚
â”‚    â”‚                    â”œâ”€â†’ docling:5001                    â”‚
â”‚    â”‚                    â””â”€â†’ tika:9998                       â”‚
â”‚    â”‚                                                         â”‚
â”‚    â”œâ”€ litellm:4000 â”€â”€â”€â”€â”€â”¬â”€â†’ postgres:5432                  â”‚
â”‚    â”‚                     â””â”€â†’ redis:6379                     â”‚
â”‚    â”‚                                                         â”‚
â”‚    â”œâ”€ searxng:8080 â”€â”€â”€â”€â”€â†’ redis:6379                       â”‚
â”‚    â”‚                                                         â”‚
â”‚    â””â”€ mcp:8000 â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ postgres:5432                    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Network Layer Responsibilities:

#### Layer 1: Tailscale (External Network)
- **Protocol:** HTTPS over Tailscale VPN
- **DNS:** MagicDNS resolves `*.your-tailnet.ts.net`
- **Auth:** Tailscale handles authentication
- **Certs:** Tailscale provides TLS certificates
- **Purpose:** Secure external access from anywhere

#### Layer 2: Caddy (Host Proxy)
- **Listens on:** Host ports 80, 443
- **Routes to:** localhost published ports
- **Example:** `ai.hostname.tailnet:443` â†’ `localhost:3000`
- **Purpose:** 
  - Terminate TLS
  - Route to correct service
  - Single entry point
  - Clean URLs

#### Layer 3: Podman Network (Internal)
- **Network:** `llm.network` (10.89.0.0/24)
- **Communication:** Container-to-container by name
- **Example:** `openwebui` â†’ `litellm:4000`
- **Purpose:**
  - Isolated network for containers
  - Service discovery by name
  - No published ports needed for internal communication

---

## Part 3: Where Does llm.network Go?

### Answer: Top Level (Shared Infrastructure)

**Location:**
```
~/.config/containers/systemd/llm.network
```

**Why top level?**
- It's **shared infrastructure**
- All services connect to it
- Not specific to any one service
- Similar to volumes that span services

**Content (`llm.network` - static, not templated):**

```ini
[Network]
NetworkName=llm
Gateway=10.89.0.1
Subnet=10.89.0.0/24
```

**In Chezmoi:**
```
private_dot_config/containers/systemd/llm.network
```
(No .tmpl extension - this is a static file)

---

## Part 4: Complete Network Flow Example

### User Accesses OpenWebUI:

```
1. User types: https://ai.llm-server.your-tailnet.ts.net
   â””â”€ Tailscale DNS resolves to your machine's Tailscale IP
   
2. HTTPS request hits Caddy (listening on :443)
   â””â”€ Caddy checks Caddyfile
   â””â”€ Matches: ai.llm-server.your-tailnet.ts.net
   
3. Caddy reverse proxies to: localhost:3000
   â””â”€ OpenWebUI container published port
   â””â”€ PublishPort=127.0.0.1:3000:8080
   
4. Inside container: OpenWebUI listens on :8080
   â””â”€ OpenWebUI needs LiteLLM
   â””â”€ Connects to: http://litellm:4000 (via llm.network)
   
5. LiteLLM container receives request on :4000
   â””â”€ LiteLLM needs database
   â””â”€ Connects to: postgresql://llm-postgres:5432 (via llm.network)
   
6. Postgres container receives connection on :5432
   â””â”€ Returns data
   
7. Response flows back through the stack
   â””â”€ LiteLLM â†’ OpenWebUI â†’ Caddy â†’ Tailscale â†’ User
```

### Key Points:

- **External access:** Uses Tailscale + Caddy
- **Internal access:** Uses `llm.network` (containers talk by name)
- **No port conflicts:** Internal ports can overlap (multiple services on :8080)
- **Published ports:** Only for Caddy to access, not for external users

---

## Part 5: Service Folder Structure Example

### OpenWebUI Service (Complete)

**Folder:** `private_dot_config/containers/systemd/openwebui/`

```
openwebui/
â”œâ”€â”€ openwebui.container.tmpl
â”œâ”€â”€ openwebui.volume
â”œâ”€â”€ openwebui.env.tmpl
â””â”€â”€ README.md (optional)
```

#### File 1: `openwebui.container.tmpl`

```ini
# Generated by Chezmoi - DO NOT EDIT MANUALLY

[Unit]
Description=Open WebUI - AI Chat Interface
After=network-online.target llm.network.service
After=litellm.service llm-postgres.service llm-redis.service
Requires=llm.network.service
Wants=litellm.service llm-postgres.service llm-redis.service

[Container]
Image=ghcr.io/open-webui/open-webui:main
AutoUpdate=registry
ContainerName=openwebui

# Network - use shared llm.network
Network=llm

# Published port for Caddy to access
PublishPort=127.0.0.1:{{ .published_ports.openwebui }}:{{ .ports.openwebui }}

# Volume
Volume=openwebui.volume:/app/backend/data:Z

# Environment from service-specific config file
EnvironmentFile=%h/.config/containers/systemd/openwebui/openwebui.env

# Health check
HealthCmd="wget --no-verbose --tries=1 --spider http://localhost:{{ .ports.openwebui }}/health || exit 1"
HealthInterval=30s
HealthTimeout=5s
HealthRetries=3
HealthStartPeriod=10s

[Service]
Slice=llm.slice
TimeoutStartSec=900
Restart=on-failure
RestartSec=10

[Install]
WantedBy=default.target
```

#### File 2: `openwebui.volume`

```ini
[Volume]
VolumeName=openwebui-data
```

#### File 3: `openwebui.env.tmpl`

```bash
# Generated by Chezmoi - DO NOT EDIT MANUALLY
# OpenWebUI Environment Configuration

# Core Settings
PORT={{ .ports.openwebui }}
WEBUI_URL={{ .urls.openwebui }}
WEBUI_NAME=AI Assistant
ENV=prod
WEBUI_AUTH=true
ENABLE_SIGNUP={{ .service_config.openwebui.enable_signup }}
DEFAULT_USER_ROLE={{ .service_config.openwebui.default_user_role }}
WEBUI_SECRET_KEY={{ .services.openwebui.secret_key }}

# Database - uses service name from llm.network
DATABASE_URL=postgresql://{{ .database.postgres_user }}:{{ .database.postgres_password }}@llm-postgres:{{ .ports.postgres }}/openwebui

# Redis - uses service name from llm.network  
REDIS_URL=redis://llm-redis:{{ .ports.redis }}/2

# LiteLLM - uses service name from llm.network
OPENAI_API_BASE_URL=http://litellm:{{ .ports.litellm }}/v1
OPENAI_API_KEY={{ .api_keys.litellm_master }}

# Document Processing - uses service names
DOCLING_SERVER_URL=http://docling:{{ .ports.docling }}
TIKA_SERVER_URL=http://tika:{{ .ports.tika }}

# Search - uses service name
SEARXNG_QUERY_URL=http://searxng:{{ .ports.searxng }}/search?q=<query>
ENABLE_RAG_WEB_SEARCH=true

# Features
ENABLE_COMMUNITY_SHARING={{ .service_config.openwebui.enable_community_sharing }}
```

**Notice:**
- âœ… Uses service names: `litellm`, `llm-postgres`, `searxng`
- âœ… Not `localhost` - that would be wrong!
- âœ… Service names work because all containers are on `llm.network`

---

## Part 6: LiteLLM Service (Complete)

**Folder:** `private_dot_config/containers/systemd/litellm/`

```
litellm/
â”œâ”€â”€ litellm.container.tmpl
â””â”€â”€ litellm.yaml.tmpl
```

#### File 1: `litellm.container.tmpl`

```ini
# Generated by Chezmoi - DO NOT EDIT MANUALLY

[Unit]
Description=LiteLLM Proxy Service
After=network-online.target llm.network.service
After=llm-postgres.service llm-redis.service
Requires=llm.network.service
Wants=llm-postgres.service llm-redis.service

[Container]
Image=ghcr.io/berriai/litellm:main-stable
AutoUpdate=registry
ContainerName=litellm

# Network
Network=llm

# Published port
PublishPort=127.0.0.1:{{ .published_ports.litellm }}:{{ .ports.litellm }}

# Mount config file from same folder
Volume=%h/.config/containers/systemd/litellm/litellm.yaml:/app/config.yaml:ro,Z

# Environment variables
Environment=LITELLM_MASTER_KEY={{ .api_keys.litellm_master }}
Environment=DATABASE_URL=postgresql://{{ .database.postgres_user }}:{{ .database.postgres_password }}@llm-postgres:{{ .ports.postgres }}/litellm
Environment=REDIS_HOST=llm-redis
Environment=REDIS_PORT={{ .ports.redis }}

# API Keys
Environment=OPENAI_API_KEY={{ .api_keys.openai }}
Environment=ANTHROPIC_API_KEY={{ .api_keys.anthropic }}
Environment=GEMINI_API_KEY={{ .api_keys.gemini }}

# Execution
Exec=--config /app/config.yaml --port {{ .ports.litellm }} --num_workers 2

# Health check
HealthCmd="curl -f http://localhost:{{ .ports.litellm }}/health/readiness || exit 1"
HealthInterval=30s
HealthTimeout=10s
HealthRetries=3

[Service]
Slice=llm.slice
TimeoutStartSec=900
Restart=on-failure
RestartSec=10

[Install]
WantedBy=default.target
```

#### File 2: `litellm.yaml.tmpl`

```yaml
# Generated by Chezmoi - DO NOT EDIT MANUALLY

model_list:
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: {{ .api_keys.openai }}
  
  - model_name: claude-3-5-sonnet
    litellm_params:
      model: anthropic/claude-3-5-sonnet-latest
      api_key: {{ .api_keys.anthropic }}
  
  - model_name: gemini-2.0-flash
    litellm_params:
      model: gemini/gemini-2.0-flash-exp
      api_key: {{ .api_keys.gemini }}

litellm_settings:
  cache: true
  cache_params:
    type: redis
    host: llm-redis
    port: {{ .ports.redis }}

general_settings:
  database_url: "postgresql://{{ .database.postgres_user }}:{{ .database.postgres_password }}@llm-postgres:{{ .ports.postgres }}/litellm"
  master_key: {{ .api_keys.litellm_master }}
```

---

## Part 7: Postgres Service (Complete)

**Folder:** `private_dot_config/containers/systemd/llm-postgres/`

```
llm-postgres/
â”œâ”€â”€ llm-postgres.container.tmpl
â””â”€â”€ llm-postgres.volume
```

#### File 1: `llm-postgres.container.tmpl`

```ini
# Generated by Chezmoi - DO NOT EDIT MANUALLY

[Unit]
Description=Shared PostgreSQL database for LLM services
After=network-online.target llm.network.service
Requires=llm.network.service

[Container]
Image=docker.io/pgvector/pgvector:pg16
AutoUpdate=registry
ContainerName=llm-postgres

# Network - NO published port, only internal access
Network=llm

# Environment
Environment=POSTGRES_USER={{ .database.postgres_user }}
Environment=POSTGRES_PASSWORD={{ .database.postgres_password }}

# Volume
Volume=llm-postgres.volume:/var/lib/postgresql/data:Z

# PostgreSQL config
Exec=-c shared_preload_libraries={{ .service_config.postgres.shared_preload_libraries }} -c max_connections={{ .service_config.postgres.max_connections }}

# Health check
HealthCmd="pg_isready -U {{ .database.postgres_user }}"
HealthInterval=30s
HealthTimeout=5s
HealthRetries=5

[Service]
Slice=llm.slice
TimeoutStartSec=900
Restart=on-failure
RestartSec=10

# Create databases
{{- range $db := .database.postgres_databases }}
ExecStartPost=/bin/bash -c 'sleep 5 && podman exec llm-postgres psql -U {{ $.database.postgres_user }} -tc "SELECT 1 FROM pg_database WHERE datname = '"'"'{{ $db }}'"'"'" | grep -q 1 || podman exec llm-postgres psql -U {{ $.database.postgres_user }} -c "CREATE DATABASE {{ $db }};"'
{{- end }}

[Install]
WantedBy=default.target
```

#### File 2: `llm-postgres.volume`

```ini
[Volume]
VolumeName=llm-postgres-data
```

**Notice:** NO `PublishPort` - Postgres is internal only!

---

## Part 8: Updated .chezmoi.yaml.tmpl (Network-aware)

```yaml
data:
  # Network configuration
  network:
    name: "llm"
    subnet: "10.89.0.0/24"
    gateway: "10.89.0.1"
  
  # Internal container ports (what they listen on INSIDE)
  ports:
    postgres: 5432
    redis: 6379
    litellm: 4000
    openwebui: 8080
    searxng: 8080
    docling: 5001
    tika: 9998
    mcp: 8000
  
  # Published ports (host:container mapping for Caddy)
  published_ports:
    openwebui: 3000      # Host 3000 â†’ Container 8080
    litellm: 4000        # Host 4000 â†’ Container 4000
    searxng: 8888        # Host 8888 â†’ Container 8080
    docling: 5001        # Host 5001 â†’ Container 5001
    redis_ui: 8001       # Host 8001 â†’ Container 8001
    # Note: postgres, tika, mcp - NO published ports (internal only)
  
  # External URLs (via Tailscale + Caddy)
  urls:
    cockpit: "https://{{ .tailscale.hostname }}.{{ .tailscale.tailnet }}"
    openwebui: "https://ai.{{ .tailscale.hostname }}.{{ .tailscale.tailnet }}"
    litellm: "https://litellm.{{ .tailscale.hostname }}.{{ .tailscale.tailnet }}"
    searxng: "https://search.{{ .tailscale.hostname }}.{{ .tailscale.tailnet }}"
```

---

## Part 9: Benefits of This Organization

### âœ… Clear Structure
- Each service has its own folder
- Easy to find all related files
- Can add per-service documentation

### âœ… Proper Network Layers
- Tailscale: External access
- Caddy: Routing to services
- llm.network: Container communication

### âœ… Maintainable
- Add new service: Create new folder
- Update service: Edit only that folder
- Shared config: Top-level files

### âœ… Secure
- Internal services (postgres, tika) have NO published ports
- Only entry point is Caddy via Tailscale
- Container network is isolated

### âœ… Scalable
- Easy to add more services
- Easy to add more databases
- Easy to reorganize

---

## Part 10: Quick Reference

### Network Troubleshooting:

```bash
# Check if llm.network exists
podman network ls | grep llm

# Inspect network
podman network inspect llm

# See which containers are on the network
podman network inspect llm | jq '.[].Containers'

# Test container-to-container communication
podman exec openwebui ping llm-postgres
podman exec openwebui curl http://litellm:4000/health
```

### Desktop Files Reference URLs:

```desktop
# openwebui.desktop.tmpl
Exec=google-chrome-stable --app={{ .urls.openwebui }}

# NOT localhost:3000 - use the Tailscale URL!
```

### Sway Keybinds Reference URLs:

```sway
# sway/config.tmpl
bindsym $mod+Shift+o exec google-chrome-stable --app={{ .urls.openwebui }}

# NOT localhost:3000 - use the Tailscale URL!
```

---

## Summary: The Three Networks

| Network | Purpose | Example | Access |
|---------|---------|---------|--------|
| **Tailscale** | External access | `ai.llm-server.tailnet.ts.net` | From anywhere on tailnet |
| **Caddy** | Proxy layer | `localhost:3000` | Host â†’ Container |
| **llm.network** | Container-to-container | `litellm:4000` | Container â†’ Container |

**Remember:**
- Desktop files & Sway â†’ Use Tailscale URLs ({{ .urls.* }})
- Caddy â†’ Proxies to localhost published ports
- Containers â†’ Talk to each other via service names on llm.network
- llm.network file â†’ Top level (shared infrastructure)

ğŸ¯ **Result:** Clean organization, proper network separation, maintainable structure!

---

# Final Clean Folder Structure - Ready to Build

## Complete Chezmoi Structure

```
~/.local/share/chezmoi/
â”‚
â”œâ”€â”€â”€ ğŸ“‹ MASTER CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚
â”œâ”€â”€ .chezmoi.yaml.tmpl              â­ Single source of truth
â”‚   â”œâ”€ network: llm, 10.89.0.0/24
â”‚   â”œâ”€ ports: Internal container ports
â”‚   â”œâ”€ published_ports: Host published ports  
â”‚   â”œâ”€ urls: Tailscale external URLs
â”‚   â””â”€ webapps: Desktop launcher definitions
â”‚
â”œâ”€â”€ encrypted_private_secrets.yaml  ğŸ”’ All secrets encrypted
â”‚   â”œâ”€ api_keys: OpenAI, Anthropic, etc.
â”‚   â”œâ”€ database: Passwords
â”‚   â”œâ”€ services: Secret keys
â”‚   â””â”€ tailscale: Auth key
â”‚
â”œâ”€â”€ .chezmoiignore                  (optional)
â”‚
â”œâ”€â”€â”€ ğŸ–¥ï¸ DESKTOP ENVIRONMENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚
â”œâ”€â”€ private_dot_local/
â”‚   â””â”€â”€ private_share/
â”‚       â””â”€â”€ private_applications/   â†’ ~/.local/share/applications/
â”‚           â”œâ”€â”€ cockpit.desktop.tmpl
â”‚           â”œâ”€â”€ openwebui.desktop.tmpl
â”‚           â”œâ”€â”€ litellm.desktop.tmpl
â”‚           â”œâ”€â”€ searxng.desktop.tmpl
â”‚           â”œâ”€â”€ nextcloud.desktop.tmpl
â”‚           â””â”€â”€ (one per webapp - templates, not auto-generated)
â”‚
â”œâ”€â”€ private_dot_config/
â”‚   â”‚
â”‚   â”œâ”€â”€ sway/                       â†’ ~/.config/sway/
â”‚   â”‚   â””â”€â”€ config.tmpl             (keybinds reference {{ .urls.* }})
â”‚   â”‚
â”‚   â”œâ”€â”€â”€ ğŸŒ EXTERNAL ACCESS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚   â”‚
â”‚   â”œâ”€â”€ caddy/                      â†’ ~/.config/caddy/
â”‚   â”‚   â””â”€â”€ Caddyfile.tmpl          â­ Reverse proxy config
â”‚   â”‚       â”œâ”€ Routes Tailscale URLs â†’ localhost ports
â”‚   â”‚       â””â”€ Each service gets subdomain
â”‚   â”‚
â”‚   â”œâ”€â”€ cockpit/                    â†’ ~/.config/cockpit/
â”‚   â”‚   â””â”€â”€ cockpit.conf.tmpl       (dashboard config)
â”‚   â”‚
â”‚   â””â”€â”€â”€ ğŸ³ CONTAINER INFRASTRUCTURE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚       â”‚
â”‚       â””â”€â”€ containers/
â”‚           â””â”€â”€ systemd/            â†’ ~/.config/containers/systemd/
â”‚               â”‚
â”‚               â”œâ”€â”€â”€ SHARED INFRASTRUCTURE (top level) â”€â”€â”€â”€â”€â”€â”€
â”‚               â”‚
â”‚               â”œâ”€â”€ llm.network     â­ SHARED NETWORK (static)
â”‚               â”‚   â””â”€ All containers connect here
â”‚               â”‚   â””â”€ 10.89.0.0/24 subnet
â”‚               â”‚   â””â”€ Service name DNS resolution
â”‚               â”‚
â”‚               â”œâ”€â”€ llm.env.tmpl    (optional: shared env vars)
â”‚               â”‚
â”‚               â”œâ”€â”€â”€ SERVICE FOLDERS (organized by service) â”€â”€
â”‚               â”‚
â”‚               â”œâ”€â”€ openwebui/      â­ MAIN UI
â”‚               â”‚   â”œâ”€â”€ openwebui.container.tmpl
â”‚               â”‚   â”‚   â”œâ”€ Network=llm
â”‚               â”‚   â”‚   â”œâ”€ PublishPort=3000:8080 (for Caddy)
â”‚               â”‚   â”‚   â””â”€ Connects to: litellm, postgres, redis
â”‚               â”‚   â”œâ”€â”€ openwebui.volume
â”‚               â”‚   â””â”€â”€ openwebui.env.tmpl
â”‚               â”‚       â””â”€ Uses service names: litellm:4000
â”‚               â”‚
â”‚               â”œâ”€â”€ litellm/        â­ LLM PROXY
â”‚               â”‚   â”œâ”€â”€ litellm.container.tmpl
â”‚               â”‚   â”‚   â”œâ”€ Network=llm
â”‚               â”‚   â”‚   â”œâ”€ PublishPort=4000:4000 (for Caddy)
â”‚               â”‚   â”‚   â””â”€ Connects to: postgres, redis
â”‚               â”‚   â””â”€â”€ litellm.yaml.tmpl
â”‚               â”‚       â””â”€ Database: llm-postgres:5432
â”‚               â”‚
â”‚               â”œâ”€â”€ llm-postgres/   â­ DATABASE
â”‚               â”‚   â”œâ”€â”€ llm-postgres.container.tmpl
â”‚               â”‚   â”‚   â”œâ”€ Network=llm
â”‚               â”‚   â”‚   â”œâ”€ NO PublishPort (internal only)
â”‚               â”‚   â”‚   â””â”€ Creates: litellm, openwebui, mcp DBs
â”‚               â”‚   â””â”€â”€ llm-postgres.volume
â”‚               â”‚
â”‚               â”œâ”€â”€ llm-redis/      â­ CACHE
â”‚               â”‚   â”œâ”€â”€ llm-redis.container.tmpl
â”‚               â”‚   â”‚   â”œâ”€ Network=llm
â”‚               â”‚   â”‚   â”œâ”€ PublishPort=8001:8001 (UI only)
â”‚               â”‚   â”‚   â””â”€ NO PublishPort for :6379 (internal)
â”‚               â”‚   â””â”€â”€ llm-redis.volume
â”‚               â”‚
â”‚               â”œâ”€â”€ searxng/        â­ SEARCH
â”‚               â”‚   â”œâ”€â”€ searxng.container.tmpl
â”‚               â”‚   â”‚   â”œâ”€ Network=llm
â”‚               â”‚   â”‚   â”œâ”€ PublishPort=8888:8080 (for Caddy)
â”‚               â”‚   â”‚   â””â”€ Connects to: redis
â”‚               â”‚   â”œâ”€â”€ searxng.volume
â”‚               â”‚   â””â”€â”€ searxng-settings.yml.tmpl
â”‚               â”‚       â””â”€ Redis: llm-redis:6379
â”‚               â”‚
â”‚               â”œâ”€â”€ docling/        â­ DOCS
â”‚               â”‚   â””â”€â”€ docling.container.tmpl
â”‚               â”‚       â”œâ”€ Network=llm
â”‚               â”‚       â””â”€ PublishPort=5001:5001 (for Caddy)
â”‚               â”‚
â”‚               â”œâ”€â”€ tika/           â­ EXTRACTION
â”‚               â”‚   â””â”€â”€ tika.container.tmpl
â”‚               â”‚       â”œâ”€ Network=llm
â”‚               â”‚       â””â”€ NO PublishPort (internal only)
â”‚               â”‚
â”‚               â”œâ”€â”€ mcp/            â­ TOOLS
â”‚               â”‚   â”œâ”€â”€ mcp.container.tmpl
â”‚               â”‚   â”‚   â”œâ”€ Network=llm
â”‚               â”‚   â”‚   â”œâ”€ NO PublishPort (internal only)
â”‚               â”‚   â”‚   â””â”€ Connects to: postgres
â”‚               â”‚   â””â”€â”€ mcp-config.json.tmpl
â”‚               â”‚       â””â”€ Database: llm-postgres:5432
â”‚               â”‚
â”‚               â””â”€â”€ user/           â†’ ~/.config/containers/systemd/user/
â”‚                   â”‚
â”‚                   â”œâ”€â”€ nextcloud/
â”‚                   â”‚   â””â”€â”€ nextcloud.container.tmpl
â”‚                   â”‚       â””â”€ PublishPort=8443:80 (for Caddy)
â”‚                   â”‚
â”‚                   â””â”€â”€ whisper/
â”‚                       â””â”€â”€ whisper-npu.container.tmpl
â”‚                           â””â”€ PublishPort=8009:5000 (for Caddy)
â”‚
â””â”€â”€â”€ ğŸ”§ AUTOMATION SCRIPTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚
    â””â”€â”€ run_once_after/
        â”œâ”€â”€ 01-setup-tailscale.sh.tmpl
        â”œâ”€â”€ 02-setup-cockpit.sh.tmpl
        â””â”€â”€ 03-reload-systemd.sh.tmpl
```

---

## Network Architecture Summary

### Three Separate Networks:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. TAILSCALE NETWORK (External)                            â”‚
â”‚    â€¢ Purpose: Secure remote access                         â”‚
â”‚    â€¢ URLs: *.llm-server.your-tailnet.ts.net               â”‚
â”‚    â€¢ Used by: Desktop files, Sway keybinds, browsers       â”‚
â”‚    â€¢ Authentication: Tailscale                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. CADDY PROXY LAYER (Host)                               â”‚
â”‚    â€¢ Listens: :443 (HTTPS)                                â”‚
â”‚    â€¢ Routes: Tailscale URL â†’ localhost published port     â”‚
â”‚    â€¢ Example: ai.llm-server.ts.net â†’ localhost:3000       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. LLM.NETWORK (Containers)                               â”‚
â”‚    â€¢ Subnet: 10.89.0.0/24                                  â”‚
â”‚    â€¢ DNS: Service name resolution                          â”‚
â”‚    â€¢ Example: openwebui â†’ litellm:4000                    â”‚
â”‚    â€¢ Purpose: Container-to-container communication         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Port Mapping Table:

| Service | Internal Port | Published Port | Caddy Access | Purpose |
|---------|--------------|----------------|--------------|---------|
| **openwebui** | 8080 | 3000 | âœ… localhost:3000 | Main UI |
| **litellm** | 4000 | 4000 | âœ… localhost:4000 | LLM API |
| **postgres** | 5432 | âŒ None | âŒ Internal only | Database |
| **redis** | 6379 | âŒ None | âŒ Internal only | Cache |
| **redis-ui** | 8001 | 8001 | âœ… localhost:8001 | Monitoring |
| **searxng** | 8080 | 8888 | âœ… localhost:8888 | Search |
| **docling** | 5001 | 5001 | âœ… localhost:5001 | Docs |
| **tika** | 9998 | âŒ None | âŒ Internal only | Extraction |
| **mcp** | 8000 | âŒ None | âŒ Internal only | Tools |

**Key:**
- **Internal Port:** What container listens on
- **Published Port:** Host port mapped to container (127.0.0.1 only)
- **Caddy Access:** Can Caddy proxy to this?
- **Internal only:** No published port, only accessible via llm.network

---

## Service Communication Examples

### Example 1: User Opens OpenWebUI

```
Browser: https://ai.llm-server.your-tailnet.ts.net
    â†“ (Tailscale VPN)
Caddy: Routes ai.llm-server... to localhost:3000
    â†“ (Published port)
OpenWebUI Container: Receives request on :8080
    â†“ (Needs LLM)
    Connects to: http://litellm:4000 (via llm.network)
    â†“ (Service name DNS)
LiteLLM Container: Receives request on :4000
```

### Example 2: LiteLLM Queries Database

```
LiteLLM: Needs to log API call
    â†“ (Connection string from config)
    Connects to: postgresql://llm-postgres:5432/litellm
    â†“ (via llm.network service name)
Postgres Container: Receives connection on :5432
    â†“ (No published port needed!)
    Returns data over llm.network
```

### Example 3: OpenWebUI Uses Search

```
OpenWebUI: User asks to search web
    â†“ (Internal request)
    Connects to: http://searxng:8080/search
    â†“ (via llm.network)
SearXNG Container: Performs search
    â†“ (Needs cache)
    Connects to: redis://llm-redis:6379
    â†“ (via llm.network)
Redis Container: Returns cached results
```

**Notice:** All internal communication uses service names, no localhost!

---

## Configuration Flow

### How a URL Appears Everywhere:

```yaml
# 1. Define ONCE in .chezmoi.yaml.tmpl
data:
  tailscale:
    hostname: "llm-server"
    tailnet: "your-tailnet.ts.net"
  
  urls:
    openwebui: "https://ai.{{ .tailscale.hostname }}.{{ .tailscale.tailnet }}"
```

```desktop
# 2. Desktop file references it
# openwebui.desktop.tmpl
Exec=google-chrome-stable --app={{ .urls.openwebui }}
```

```sway
# 3. Sway config references it
# sway/config.tmpl
bindsym $mod+Shift+o exec google-chrome-stable --app={{ .urls.openwebui }}
```

```ini
# 4. Container env references it
# openwebui/openwebui.env.tmpl
WEBUI_URL={{ .urls.openwebui }}
```

```caddyfile
# 5. Caddy routes it
# Caddyfile.tmpl
ai.{{ .tailscale.hostname }}.{{ .tailscale.tailnet }}:443 {
    reverse_proxy localhost:{{ .published_ports.openwebui }}
}
```

**Result:** Change subdomain once â†’ updates in 5 places automatically!

---

## Build Order (Sequential)

### Phase 1: Foundation (Week 1)
```bash
# Files to create:
1. .chezmoi.yaml.tmpl
2. encrypted_private_secrets.yaml
3. Caddyfile.tmpl
4. cockpit.conf.tmpl
5. Desktop files (5-8 .desktop.tmpl files)

# Test:
chezmoi apply -v
```

### Phase 2: Network & Shared (Week 1)
```bash
# Files to create:
6. llm.network (top level)
7. llm.env.tmpl (optional)

# Test:
podman network create llm --subnet 10.89.0.0/24
```

### Phase 3: Databases (Week 1-2)
```bash
# Folders to create:
8. llm-postgres/ folder + 2 files
9. llm-redis/ folder + 2 files

# Test:
systemctl --user start llm-postgres llm-redis
```

### Phase 4: Core Services (Week 2)
```bash
# Folders to create:
10. litellm/ folder + 2 files
11. openwebui/ folder + 3 files

# Test:
systemctl --user start litellm openwebui
```

### Phase 5: Supporting Services (Week 2-3)
```bash
# Folders to create:
12. searxng/ folder + 3 files
13. docling/ folder + 1 file
14. tika/ folder + 1 file
15. mcp/ folder + 2 files

# Test:
systemctl --user start searxng docling tika mcp
```

### Phase 6: User Services (Week 3)
```bash
# Folders to create:
16. user/nextcloud/ folder + 1 file
17. user/whisper/ folder + 1 file

# Test:
systemctl --user start nextcloud whisper
```

---

## Quick Commands

### Apply All Changes:
```bash
chezmoi apply -v
systemctl --user daemon-reload
```

### Test Network:
```bash
# Check network exists
podman network ls | grep llm

# Test container communication
podman exec openwebui ping litellm
podman exec openwebui curl http://litellm:4000/health
```

### Test External Access:
```bash
# From browser (anywhere on Tailscale):
https://ai.llm-server.your-tailnet.ts.net
```

### Debug:
```bash
# See generated file before applying
chezmoi cat ~/.config/containers/systemd/openwebui/openwebui.container

# See all configuration data
chezmoi data | jq .
```

---

## Key Takeaways

1. âœ… **Each service = one folder**
   - Clear organization
   - Easy to maintain
   - Can add per-service docs

2. âœ… **llm.network at top level**
   - Shared infrastructure
   - All services use it
   - Not specific to any service

3. âœ… **Three network layers**
   - Tailscale: External access
   - Caddy: Routing proxy
   - llm.network: Container communication

4. âœ… **Desktop files as templates**
   - Not auto-generated
   - Reference {{ .urls.* }}
   - Always point to correct URL

5. âœ… **Service names for internal communication**
   - Use `litellm:4000`, not `localhost:4000`
   - Works because of llm.network
   - Containers find each other by name

6. âœ… **Published ports only when needed**
   - For Caddy to access: Yes
   - For internal only: No
   - Reduces attack surface

---

ğŸ¯ **This structure is production-ready, maintainable, and scales well!**
