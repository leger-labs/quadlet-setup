This is the crux of the local ai experience. We find multiple options in:
- ramalama
- optimized amd strix halo toolboxes
- local llama.cpp (aka llama-serve)

for driver-enabled optimization
https://github.com/kyuz0/amd-strix-halo-toolboxes

investigate llama-swap in this context: https://www.reddit.com/r/LocalLLaMA/comments/1mon08l/tutorial_open_webui_and_llamaswap_works_great/
https://docs.rancherdesktop.io/tutorials/working-with-llms/

Current understanding:
- one container per model
- managed by llama-swap

https://github.com/kyuz0/amd-strix-halo-toolboxes
https://strixhalo-homelab.d7.wtf/AI/AI-Capabilities-Overview
https://llm-tracker.info/_TOORG/Strix-Halo
https://github.com/lhl/strix-halo-testing/tree/main
https://github.com/kyuz0/amd-strix-halo-toolboxes/blob/main/docs/building.md
Qwen3-235B in Q3_K_XL and GLM 4.5-Air-106B in Q6_K_XL
