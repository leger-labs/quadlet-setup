This is the crux of the local ai experience. We find multiple options in:
- ramalama
- optimized amd strix halo toolboxes
- local llama.cpp (aka llama-serve)

for driver-enabled optimization
https://github.com/kyuz0/amd-strix-halo-toolboxes

investigate llama-swap in this context: https://www.reddit.com/r/LocalLLaMA/comments/1mon08l/tutorial_open_webui_and_llamaswap_works_great/
https://docs.rancherdesktop.io/tutorials/working-with-llms/

Current understanding:
- one container per model
- managed by llama-swap
