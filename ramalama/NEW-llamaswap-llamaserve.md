The architecture should be:

**RamaLama**: Host binary (installed via DNF), configured through dotfiles
**llama-swap**: Podman quadlet container with mounted Podman socket

---

## Corrected Architecture

```
Host:
├─ ramalama (binary)           → Manages models in ~/.local/share/ramalama
├─ podman.sock                 → Controls containers
│
Containers:
├─ llama-swap (quadlet)        → Routes requests, spawns RamaLama containers
│  └─ Mounts: podman.sock, ramalama models
│
├─ ramalama-qwen3-32b          → Spawned by llama-swap
├─ ramalama-smollm-135m        → Spawned by llama-swap
│
└─ litellm (quadlet)           → Routes to llama-swap for local models
```

---

## Full Implementation (Corrected)

### 1. Install RamaLama on Host
### 2. RamaLama Host Configuration

**File:** `dot_config/ramalama/ramalama.conf.tmpl`

```toml
# Generated by Chezmoi - DO NOT EDIT MANUALLY
# RamaLama configuration for host

[[ramalama]]

# Use containers by default
container = true

# Container engine (podman on Fedora)
engine = "podman"

# Default runtime
runtime = "llama.cpp"

# Use vulkan image for AMD GPU
[[ramalama.images]]
  GGML_VK_VISIBLE_DEVICES = "quay.io/ramalama/ramalama:latest"

# Model storage
store = "{{ .chezmoi.homeDir }}/.local/share/ramalama"

# Network for RamaLama containers
# (they'll connect to llm.network)
```

---

### 3. llama-swap Container (Quadlet)

**Folder:** `private_dot_config/containers/systemd/llama-swap/`

#### `llama-swap.container.tmpl`

```ini
# Generated by Chezmoi - DO NOT EDIT MANUALLY
# llama-swap - Local LLM router (Podman quadlet)

[Unit]
Description=llama-swap - Local LLM Model Router
After=network-online.target llm.network.service
Requires=llm.network.service

[Container]
Image=ghcr.io/mostlygeek/llama-swap:cpu
AutoUpdate=registry
ContainerName=llama-swap

# Network - internal + published port for LiteLLM and Caddy
Network=llm.network
PublishPort=127.0.0.1:{{ .published_ports.llama_swap }}:8080

# Mount Podman socket (for spawning RamaLama containers)
Volume=/run/user/%U/podman/podman.sock:/run/podman/podman.sock:Z

# Mount RamaLama model cache (read-only)
Volume={{ .chezmoi.homeDir }}/.local/share/ramalama:/root/.local/share/ramalama:ro,Z

# Mount config
Volume=%h/.config/containers/systemd/llama-swap/config.yaml:/app/config.yaml:ro,Z

# Data volume for llama-swap state
Volume=llama-swap.volume:/app/data:Z

# Allow container to control podman
PodmanArgs=--security-opt=label=disable

# Environment
Environment=PODMAN_SOCK=unix:///run/podman/podman.sock
Environment=HOME=/root

# Health check
HealthCmd="curl -f http://localhost:8080/health || exit 1"
HealthInterval=30s
HealthTimeout=5s
HealthRetries=3
HealthStartPeriod=10s

[Service]
Slice=llm.slice
TimeoutStartSec=300
Restart=on-failure
RestartSec=10

[Install]
WantedBy=scroll-session.target
```

#### `llama-swap.volume`

```ini
[Volume]
VolumeName=llama-swap-data
```

#### `config.yaml.tmpl`

```yaml
# Generated by Chezmoi - DO NOT EDIT MANUALLY
# llama-swap configuration - routes to RamaLama containers

# Allow time for model downloads
healthCheckTimeout: 3600

models:
{{- range .local_models }}
  "{{ .name }}":
    # Proxy to RamaLama container
    proxy: "http://ramalama-{{ .name }}:8080"
    
    # TTL: Unload after inactivity (seconds)
    ttl: {{ .ttl }}
    
    # Start RamaLama container (via podman from inside llama-swap container)
    cmd: >
      podman run -d
      --name ramalama-{{ .name }}
      --network llm
      --device /dev/dri
      --device /dev/kfd
      --security-opt label=disable
      -v {{ `{{ .chezmoi.homeDir }}` }}/.local/share/ramalama:/root/.local/share/ramalama:Z
      -e RAMALAMA_IN_CONTAINER=true
      quay.io/ramalama/ramalama:latest
      serve
      --runtime llama.cpp
      --host 0.0.0.0
      --port 8080
      --ngl {{ .ngl }}
      --ctx-size {{ .ctx_size }}
      {{ .model }}
    
    # Stop RamaLama container
    cmdStop: >
      podman stop -t 10 ramalama-{{ .name }} &&
      podman rm ramalama-{{ .name }}
    
    # Metadata
    description: "{{ .description }}"
{{- end }}
```

#### `README.md`

```markdown
# llama-swap - Local LLM Router

## Overview

llama-swap runs as a **Podman quadlet container** and dynamically manages RamaLama containers for local LLM inference using llama.cpp with Vulkan GPU acceleration.

## Architecture

```
Request Flow:
User → OpenWebUI → LiteLLM:4000 → llama-swap:9292 → RamaLama containers

Container Communication:
llama-swap (container)
    ├─ Mounted: /run/podman/podman.sock (controls host podman)
    ├─ Mounted: ~/.local/share/ramalama (model cache)
    └─ Network: llm.network (talks to RamaLama containers)

RamaLama containers (spawned by llama-swap):
    ├─ ramalama-qwen3-32b:8080 (on llm.network)
    ├─ ramalama-smollm-135m:8080 (on llm.network)
    └─ ... (created on-demand)
```

## How It Works

1. **Request arrives**: User asks for "qwen3-32b"
2. **llama-swap checks**: Is container running?
   - NO → Executes `cmd` via mounted podman socket
   - Container starts: `ramalama-qwen3-32b`
3. **Route request**: Proxies to `http://ramalama-qwen3-32b:8080`
4. **Serve response**: llama.cpp/llama-server handles inference
5. **After TTL**: No requests for `ttl` seconds → executes `cmdStop`

## Configuration

Models defined in `.chezmoi.yaml.tmpl` under `local_models`:

- **name**: Model identifier
- **model**: RamaLama model URI (e.g., `ollama://qwen3-32b`)
- **port**: Internal container port (always 8080 for RamaLama)
- **ctx_size**: Context window
- **ngl**: GPU layers (999 = all)
- **ttl**: Seconds before auto-unload
- **description**: Human description

## Model Management

Use **RamaLama CLI on the host** for model operations:

```bash
# Pre-download models (speeds up first request)
ramalama pull ollama://qwen3-32b
ramalama pull ollama://smollm-135m

# List downloaded models
ramalama list

# Inspect model
ramalama inspect qwen3-32b

# Remove model
ramalama rm qwen3-32b

# Check RamaLama version
ramalama version
```

## Service Control

```bash
# Start llama-swap
systemctl --user start llama-swap

# Status
systemctl --user status llama-swap

# Logs
journalctl --user -u llama-swap -f

# Enable on boot
systemctl --user enable llama-swap
```

## Web UI

**Access**: `https://llama-swap.{{ .tailscale.hostname }}.{{ .tailscale.tailnet }}`

**Features**:
- View running RamaLama containers
- Monitor request activity
- Manually unload models
- Stream logs
- Performance metrics

## Manual Container Control

```bash
# View RamaLama containers spawned by llama-swap
podman ps --filter name=ramalama-

# View logs from specific model
podman logs ramalama-qwen3-32b -f

# Manually stop a model (llama-swap will respawn on next request)
podman stop ramalama-qwen3-32b

# Resource usage
podman stats ramalama-qwen3-32b
```

## API Endpoints

### OpenAI-compatible
- `POST /v1/chat/completions`
- `POST /v1/completions`
- `GET /v1/models`

### llama-swap specific
- `GET /ui` - Web interface
- `GET /health` - Health check
- `GET /running` - List running models
- `POST /unload` - Manually unload model
- `GET /logs/stream` - Stream logs

## Testing

```bash
# List models via llama-swap
curl http://localhost:9292/v1/models | jq

# Chat (auto-starts container)
curl -X POST http://localhost:9292/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "smollm-135m",
    "messages": [{"role": "user", "content": "Hello!"}]
  }' | jq -r '.choices[0].message.content'

# Check running containers
curl http://localhost:9292/running | jq
```

## GPU Access

RamaLama containers have access to AMD GPU via:
- `/dev/dri` - Direct Rendering Infrastructure
- `/dev/kfd` - Kernel Fusion Driver
- Vulkan backend in llama.cpp (automatic)

## Troubleshooting

### Model won't start
```bash
# Check llama-swap logs
journalctl --user -u llama-swap -f

# Check if podman socket is accessible
podman --remote ps

# Try spawning container manually
podman run -it --rm quay.io/ramalama/ramalama:latest version
```

### Container spawn fails
- Verify podman socket mounted: `podman exec llama-swap ls /run/podman/`
- Check SELinux: `ausearch -m avc -ts recent`
- Verify model cache: `ls ~/.local/share/ramalama/models`

### Out of VRAM
- Reduce `ngl` (number of GPU layers)
- Reduce `ctx_size`
- Lower `ttl` for faster unloading
- Only use one large model at a time

## Multiple Models

llama-swap can run multiple RamaLama containers simultaneously (VRAM permitting):

```bash
# Request model A
curl http://localhost:9292/v1/chat/completions -d '{"model":"qwen3-32b",...}'
# → Spawns ramalama-qwen3-32b container

# Request model B (while A still loaded)
curl http://localhost:9292/v1/chat/completions -d '{"model":"smollm-135m",...}'
# → Spawns ramalama-smollm-135m container

# Both now running
podman ps --filter name=ramalama-
```

Models auto-unload after TTL expires.
```

---

### 4. Update Caddyfile

**File:** `private_dot_config/caddy/Caddyfile.tmpl`

Add llama-swap UI route:

```caddyfile
# ... existing routes ...

# llama-swap Web UI
llama-swap.{{ .tailscale.hostname }}.{{ .tailscale.tailnet }}:443 {
    reverse_proxy localhost:{{ .published_ports.llama_swap }}
}

# LiteLLM UI
litellm-ui.{{ .tailscale.hostname }}.{{ .tailscale.tailnet }}:443 {
    reverse_proxy localhost:{{ .published_ports.litellm }}
}
```

---

### 5. Desktop Launcher

**File:** `private_dot_local/private_share/private_applications/llama-swap.desktop.tmpl`

```desktop
[Desktop Entry]
Version=1.0
Type=Application
Name=llama-swap Dashboard
Comment=Local LLM Model Manager
Exec=google-chrome-stable --app={{ .urls.llama_swap }}
Icon=dashboard
Terminal=false
Categories=Development;AI;
```

---

### 6. Individual RamaLama Model Files (Optional)

Since you're comfortable creating individual files, here are templates for direct RamaLama quadlets (alternative to llama-swap management):

**Folder:** `private_dot_config/containers/systemd/ramalama/`

#### `ramalama-qwen3-32b.container.tmpl`

```ini
# Manual RamaLama container (alternative to llama-swap)
# Start manually: systemctl --user start ramalama-qwen3-32b

[Unit]
Description=RamaLama - Qwen3 32B
After=network-online.target llm.network.service
Requires=llm.network.service

[Container]
Image=quay.io/ramalama/ramalama:latest
AutoUpdate=registry
ContainerName=ramalama-qwen3-32b

# Network
Network=llm.network

# GPU access
AddDevice=/dev/dri
AddDevice=/dev/kfd
PodmanArgs=--security-opt=label=disable

# Model cache
Volume={{ .chezmoi.homeDir }}/.local/share/ramalama:/root/.local/share/ramalama:Z

# Environment
Environment=RAMALAMA_IN_CONTAINER=true

# Start serving
Exec=serve --runtime llama.cpp --host 0.0.0.0 --port 8080 --ngl 999 --ctx-size 8192 ollama://qwen3-32b

# Health check
HealthCmd="curl -f http://localhost:8080/health || exit 1"
HealthInterval=30s
HealthTimeout=5s
HealthRetries=3
HealthStartPeriod=60s

[Service]
Slice=llm.slice
TimeoutStartSec=900
Restart=on-failure
RestartSec=10

[Install]
# Don't auto-start (managed by llama-swap)
# WantedBy=scroll-session.target
```

*(Create similar files for other models if you want manual control)*

---

## Updated File Structure

```
~/.local/share/chezmoi/
│
├── run_once_before/
│   └── 00-install-ramalama.sh.tmpl         ⭐ Install RamaLama on host
│
├── dot_config/ramalama/
│   └── ramalama.conf.tmpl                   ⭐ RamaLama host config
│
├── private_dot_config/
│   │
│   ├── caddy/
│   │   └── Caddyfile.tmpl                   (updated with llama-swap route)
│   │
│   └── containers/systemd/
│       │
│       ├── llama-swap/                      ⭐ NEW - Quadlet container
│       │   ├── llama-swap.container.tmpl
│       │   ├── llama-swap.volume
│       │   ├── config.yaml.tmpl
│       │   └── README.md
│       │
│       ├── ramalama/                        ⭐ OPTIONAL - Manual containers
│       │   ├── ramalama-qwen3-32b.container.tmpl
│       │   └── ramalama-smollm-135m.container.tmpl
│       │
│       └── litellm/
│           └── litellm.yaml.tmpl            (already updated)
│
└── private_dot_local/private_share/private_applications/
    └── llama-swap.desktop.tmpl              ⭐ NEW
```

---

## Complete Setup Flow

### 1. Initial Setup

```bash
# Apply chezmoi (installs RamaLama, creates configs)
chezmoi apply -v

# Verify RamaLama installed
ramalama version

# Pre-download models (optional but recommended)
ramalama pull ollama://qwen3-32b
ramalama pull ollama://smollm-135m

# List downloaded models
ramalama list
```

### 2. Start Services

```bash
# Reload systemd
systemctl --user daemon-reload

# Start llama-swap (quadlet)
systemctl --user start llama-swap

# Check status
systemctl --user status llama-swap

# Enable on boot
systemctl --user enable llama-swap
```

### 3. Test

```bash
# Test llama-swap directly
curl http://localhost:9292/v1/models | jq

# Test via LiteLLM
curl http://localhost:4000/v1/models \
  -H "Authorization: Bearer sk-your-key" | jq

# Make a request (auto-starts RamaLama container)
curl -X POST http://localhost:9292/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "smollm-135m",
    "messages": [{"role": "user", "content": "Hello!"}]
  }' | jq

# Check running containers
podman ps --filter name=ramalama-
```

### 4. Access UIs

- **llama-swap**: `https://llama-swap.llm-server.your-tailnet.ts.net`
- **LiteLLM**: `https://litellm-ui.llm-server.your-tailnet.ts.net`
- **OpenWebUI**: `https://ai.llm-server.your-tailnet.ts.net`

---

## Why This Architecture is Correct

1. ✅ **RamaLama on host**: Native Fedora package, manages model downloads
2. ✅ **llama-swap in container**: Proper Podman quadlet, exposed via Caddy
3. ✅ **Podman socket mounting**: Enables container-to-container management
4. ✅ **llm.network**: All containers communicate via shared network
5. ✅ **Vulkan GPU**: Automatic AMD acceleration via RamaLama images
6. ✅ **Dynamic loading**: Models spawn on-demand
7. ✅ **Fedora Atomic native**: Uses Fedora's tooling properly

This is exactly the "Fedora Atomic way" - host binaries + containerized services working together! 🎯

---

Additional Node:
the specific model/provider selection should also be configured from the chezmoi.yaml.tmpl file
meaning that when i want to add a new model to my local ramalama/llama-swap stack it will be done through this single file and it will get propagated throughout. 

minor note: 
the models defined here should also be used for embedding selection, and the task models of openwebui. meaning that ramalama is the only llm runner on my system
