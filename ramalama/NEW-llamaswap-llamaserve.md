The architecture should be:

**RamaLama**: Host binary (installed via DNF), configured through dotfiles
**llama-swap**: Podman quadlet container with mounted Podman socket

---

## Corrected Architecture

```
Host:
‚îú‚îÄ ramalama (binary)           ‚Üí Manages models in ~/.local/share/ramalama
‚îú‚îÄ podman.sock                 ‚Üí Controls containers
‚îÇ
Containers:
‚îú‚îÄ llama-swap (quadlet)        ‚Üí Routes requests, spawns RamaLama containers
‚îÇ  ‚îî‚îÄ Mounts: podman.sock, ramalama models
‚îÇ
‚îú‚îÄ ramalama-qwen3-32b          ‚Üí Spawned by llama-swap
‚îú‚îÄ ramalama-smollm-135m        ‚Üí Spawned by llama-swap
‚îÇ
‚îî‚îÄ litellm (quadlet)           ‚Üí Routes to llama-swap for local models
```

---

## Full Implementation (Corrected)

### 1. Install RamaLama on Host
### 2. RamaLama Host Configuration

**File:** `dot_config/ramalama/ramalama.conf.tmpl`

```toml
# Generated by Chezmoi - DO NOT EDIT MANUALLY
# RamaLama configuration for host

[[ramalama]]

# Use containers by default
container = true

# Container engine (podman on Fedora)
engine = "podman"

# Default runtime
runtime = "llama.cpp"

# Use vulkan image for AMD GPU
[[ramalama.images]]
  GGML_VK_VISIBLE_DEVICES = "quay.io/ramalama/ramalama:latest"

# Model storage
store = "{{ .chezmoi.homeDir }}/.local/share/ramalama"

# Network for RamaLama containers
# (they'll connect to llm.network)
```

---

### 3. llama-swap Container (Quadlet)

**Folder:** `private_dot_config/containers/systemd/llama-swap/`

#### `llama-swap.container.tmpl`

```ini
# Generated by Chezmoi - DO NOT EDIT MANUALLY
# llama-swap - Local LLM router (Podman quadlet)

[Unit]
Description=llama-swap - Local LLM Model Router
After=network-online.target llm.network.service
Requires=llm.network.service

[Container]
Image=ghcr.io/mostlygeek/llama-swap:cpu
AutoUpdate=registry
ContainerName=llama-swap

# Network - internal + published port for LiteLLM and Caddy
Network=llm.network
PublishPort=127.0.0.1:{{ .published_ports.llama_swap }}:8080

# Mount Podman socket (for spawning RamaLama containers)
Volume=/run/user/%U/podman/podman.sock:/run/podman/podman.sock:Z

# Mount RamaLama model cache (read-only)
Volume={{ .chezmoi.homeDir }}/.local/share/ramalama:/root/.local/share/ramalama:ro,Z

# Mount config
Volume=%h/.config/containers/systemd/llama-swap/config.yaml:/app/config.yaml:ro,Z

# Data volume for llama-swap state
Volume=llama-swap.volume:/app/data:Z

# Allow container to control podman
PodmanArgs=--security-opt=label=disable

# Environment
Environment=PODMAN_SOCK=unix:///run/podman/podman.sock
Environment=HOME=/root

# Health check
HealthCmd="curl -f http://localhost:8080/health || exit 1"
HealthInterval=30s
HealthTimeout=5s
HealthRetries=3
HealthStartPeriod=10s

[Service]
Slice=llm.slice
TimeoutStartSec=300
Restart=on-failure
RestartSec=10

[Install]
WantedBy=scroll-session.target
```

#### `llama-swap.volume`

```ini
[Volume]
VolumeName=llama-swap-data
```

#### `config.yaml.tmpl`

```yaml
# Generated by Chezmoi - DO NOT EDIT MANUALLY
# llama-swap configuration - routes to RamaLama containers

# Allow time for model downloads
healthCheckTimeout: 3600

models:
{{- range .local_models }}
  "{{ .name }}":
    # Proxy to RamaLama container
    proxy: "http://ramalama-{{ .name }}:8080"
    
    # TTL: Unload after inactivity (seconds)
    ttl: {{ .ttl }}
    
    # Start RamaLama container (via podman from inside llama-swap container)
    cmd: >
      podman run -d
      --name ramalama-{{ .name }}
      --network llm
      --device /dev/dri
      --device /dev/kfd
      --security-opt label=disable
      -v {{ `{{ .chezmoi.homeDir }}` }}/.local/share/ramalama:/root/.local/share/ramalama:Z
      -e RAMALAMA_IN_CONTAINER=true
      quay.io/ramalama/ramalama:latest
      serve
      --runtime llama.cpp
      --host 0.0.0.0
      --port 8080
      --ngl {{ .ngl }}
      --ctx-size {{ .ctx_size }}
      {{ .model }}
    
    # Stop RamaLama container
    cmdStop: >
      podman stop -t 10 ramalama-{{ .name }} &&
      podman rm ramalama-{{ .name }}
    
    # Metadata
    description: "{{ .description }}"
{{- end }}
```

#### `README.md`

```markdown
# llama-swap - Local LLM Router

## Overview

llama-swap runs as a **Podman quadlet container** and dynamically manages RamaLama containers for local LLM inference using llama.cpp with Vulkan GPU acceleration.

## Architecture

```
Request Flow:
User ‚Üí OpenWebUI ‚Üí LiteLLM:4000 ‚Üí llama-swap:9292 ‚Üí RamaLama containers

Container Communication:
llama-swap (container)
    ‚îú‚îÄ Mounted: /run/podman/podman.sock (controls host podman)
    ‚îú‚îÄ Mounted: ~/.local/share/ramalama (model cache)
    ‚îî‚îÄ Network: llm.network (talks to RamaLama containers)

RamaLama containers (spawned by llama-swap):
    ‚îú‚îÄ ramalama-qwen3-32b:8080 (on llm.network)
    ‚îú‚îÄ ramalama-smollm-135m:8080 (on llm.network)
    ‚îî‚îÄ ... (created on-demand)
```

## How It Works

1. **Request arrives**: User asks for "qwen3-32b"
2. **llama-swap checks**: Is container running?
   - NO ‚Üí Executes `cmd` via mounted podman socket
   - Container starts: `ramalama-qwen3-32b`
3. **Route request**: Proxies to `http://ramalama-qwen3-32b:8080`
4. **Serve response**: llama.cpp/llama-server handles inference
5. **After TTL**: No requests for `ttl` seconds ‚Üí executes `cmdStop`

## Configuration

Models defined in `.chezmoi.yaml.tmpl` under `local_models`:

- **name**: Model identifier
- **model**: RamaLama model URI (e.g., `ollama://qwen3-32b`)
- **port**: Internal container port (always 8080 for RamaLama)
- **ctx_size**: Context window
- **ngl**: GPU layers (999 = all)
- **ttl**: Seconds before auto-unload
- **description**: Human description

## Model Management

Use **RamaLama CLI on the host** for model operations:

```bash
# Pre-download models (speeds up first request)
ramalama pull ollama://qwen3-32b
ramalama pull ollama://smollm-135m

# List downloaded models
ramalama list

# Inspect model
ramalama inspect qwen3-32b

# Remove model
ramalama rm qwen3-32b

# Check RamaLama version
ramalama version
```

## Service Control

```bash
# Start llama-swap
systemctl --user start llama-swap

# Status
systemctl --user status llama-swap

# Logs
journalctl --user -u llama-swap -f

# Enable on boot
systemctl --user enable llama-swap
```

## Web UI

**Access**: `https://llama-swap.{{ .tailscale.hostname }}.{{ .tailscale.tailnet }}`

**Features**:
- View running RamaLama containers
- Monitor request activity
- Manually unload models
- Stream logs
- Performance metrics

## Manual Container Control

```bash
# View RamaLama containers spawned by llama-swap
podman ps --filter name=ramalama-

# View logs from specific model
podman logs ramalama-qwen3-32b -f

# Manually stop a model (llama-swap will respawn on next request)
podman stop ramalama-qwen3-32b

# Resource usage
podman stats ramalama-qwen3-32b
```

## API Endpoints

### OpenAI-compatible
- `POST /v1/chat/completions`
- `POST /v1/completions`
- `GET /v1/models`

### llama-swap specific
- `GET /ui` - Web interface
- `GET /health` - Health check
- `GET /running` - List running models
- `POST /unload` - Manually unload model
- `GET /logs/stream` - Stream logs

## Testing

```bash
# List models via llama-swap
curl http://localhost:9292/v1/models | jq

# Chat (auto-starts container)
curl -X POST http://localhost:9292/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "smollm-135m",
    "messages": [{"role": "user", "content": "Hello!"}]
  }' | jq -r '.choices[0].message.content'

# Check running containers
curl http://localhost:9292/running | jq
```

## GPU Access

RamaLama containers have access to AMD GPU via:
- `/dev/dri` - Direct Rendering Infrastructure
- `/dev/kfd` - Kernel Fusion Driver
- Vulkan backend in llama.cpp (automatic)

## Troubleshooting

### Model won't start
```bash
# Check llama-swap logs
journalctl --user -u llama-swap -f

# Check if podman socket is accessible
podman --remote ps

# Try spawning container manually
podman run -it --rm quay.io/ramalama/ramalama:latest version
```

### Container spawn fails
- Verify podman socket mounted: `podman exec llama-swap ls /run/podman/`
- Check SELinux: `ausearch -m avc -ts recent`
- Verify model cache: `ls ~/.local/share/ramalama/models`

### Out of VRAM
- Reduce `ngl` (number of GPU layers)
- Reduce `ctx_size`
- Lower `ttl` for faster unloading
- Only use one large model at a time

## Multiple Models

llama-swap can run multiple RamaLama containers simultaneously (VRAM permitting):

```bash
# Request model A
curl http://localhost:9292/v1/chat/completions -d '{"model":"qwen3-32b",...}'
# ‚Üí Spawns ramalama-qwen3-32b container

# Request model B (while A still loaded)
curl http://localhost:9292/v1/chat/completions -d '{"model":"smollm-135m",...}'
# ‚Üí Spawns ramalama-smollm-135m container

# Both now running
podman ps --filter name=ramalama-
```

Models auto-unload after TTL expires.
```

---

### 4. Update Caddyfile

**File:** `private_dot_config/caddy/Caddyfile.tmpl`

Add llama-swap UI route:

```caddyfile
# ... existing routes ...

# llama-swap Web UI
llama-swap.{{ .tailscale.hostname }}.{{ .tailscale.tailnet }}:443 {
    reverse_proxy localhost:{{ .published_ports.llama_swap }}
}

# LiteLLM UI
litellm-ui.{{ .tailscale.hostname }}.{{ .tailscale.tailnet }}:443 {
    reverse_proxy localhost:{{ .published_ports.litellm }}
}
```

---

### 5. Desktop Launcher

**File:** `private_dot_local/private_share/private_applications/llama-swap.desktop.tmpl`

```desktop
[Desktop Entry]
Version=1.0
Type=Application
Name=llama-swap Dashboard
Comment=Local LLM Model Manager
Exec=google-chrome-stable --app={{ .urls.llama_swap }}
Icon=dashboard
Terminal=false
Categories=Development;AI;
```

---

### 6. Individual RamaLama Model Files (Optional)

Since you're comfortable creating individual files, here are templates for direct RamaLama quadlets (alternative to llama-swap management):

**Folder:** `private_dot_config/containers/systemd/ramalama/`

#### `ramalama-qwen3-32b.container.tmpl`

```ini
# Manual RamaLama container (alternative to llama-swap)
# Start manually: systemctl --user start ramalama-qwen3-32b

[Unit]
Description=RamaLama - Qwen3 32B
After=network-online.target llm.network.service
Requires=llm.network.service

[Container]
Image=quay.io/ramalama/ramalama:latest
AutoUpdate=registry
ContainerName=ramalama-qwen3-32b

# Network
Network=llm.network

# GPU access
AddDevice=/dev/dri
AddDevice=/dev/kfd
PodmanArgs=--security-opt=label=disable

# Model cache
Volume={{ .chezmoi.homeDir }}/.local/share/ramalama:/root/.local/share/ramalama:Z

# Environment
Environment=RAMALAMA_IN_CONTAINER=true

# Start serving
Exec=serve --runtime llama.cpp --host 0.0.0.0 --port 8080 --ngl 999 --ctx-size 8192 ollama://qwen3-32b

# Health check
HealthCmd="curl -f http://localhost:8080/health || exit 1"
HealthInterval=30s
HealthTimeout=5s
HealthRetries=3
HealthStartPeriod=60s

[Service]
Slice=llm.slice
TimeoutStartSec=900
Restart=on-failure
RestartSec=10

[Install]
# Don't auto-start (managed by llama-swap)
# WantedBy=scroll-session.target
```

*(Create similar files for other models if you want manual control)*

---

## Updated File Structure

```
~/.local/share/chezmoi/
‚îÇ
‚îú‚îÄ‚îÄ run_once_before/
‚îÇ   ‚îî‚îÄ‚îÄ 00-install-ramalama.sh.tmpl         ‚≠ê Install RamaLama on host
‚îÇ
‚îú‚îÄ‚îÄ dot_config/ramalama/
‚îÇ   ‚îî‚îÄ‚îÄ ramalama.conf.tmpl                   ‚≠ê RamaLama host config
‚îÇ
‚îú‚îÄ‚îÄ private_dot_config/
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ caddy/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Caddyfile.tmpl                   (updated with llama-swap route)
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ containers/systemd/
‚îÇ       ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ llama-swap/                      ‚≠ê NEW - Quadlet container
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ llama-swap.container.tmpl
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ llama-swap.volume
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ config.yaml.tmpl
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ       ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ ramalama/                        ‚≠ê OPTIONAL - Manual containers
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ ramalama-qwen3-32b.container.tmpl
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ramalama-smollm-135m.container.tmpl
‚îÇ       ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ litellm/
‚îÇ           ‚îî‚îÄ‚îÄ litellm.yaml.tmpl            (already updated)
‚îÇ
‚îî‚îÄ‚îÄ private_dot_local/private_share/private_applications/
    ‚îî‚îÄ‚îÄ llama-swap.desktop.tmpl              ‚≠ê NEW
```

---

## Complete Setup Flow

### 1. Initial Setup

```bash
# Apply chezmoi (installs RamaLama, creates configs)
chezmoi apply -v

# Verify RamaLama installed
ramalama version

# Pre-download models (optional but recommended)
ramalama pull ollama://qwen3-32b
ramalama pull ollama://smollm-135m

# List downloaded models
ramalama list
```

### 2. Start Services

```bash
# Reload systemd
systemctl --user daemon-reload

# Start llama-swap (quadlet)
systemctl --user start llama-swap

# Check status
systemctl --user status llama-swap

# Enable on boot
systemctl --user enable llama-swap
```

### 3. Test

```bash
# Test llama-swap directly
curl http://localhost:9292/v1/models | jq

# Test via LiteLLM
curl http://localhost:4000/v1/models \
  -H "Authorization: Bearer sk-your-key" | jq

# Make a request (auto-starts RamaLama container)
curl -X POST http://localhost:9292/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "smollm-135m",
    "messages": [{"role": "user", "content": "Hello!"}]
  }' | jq

# Check running containers
podman ps --filter name=ramalama-
```

### 4. Access UIs

- **llama-swap**: `https://llama-swap.llm-server.your-tailnet.ts.net`
- **LiteLLM**: `https://litellm-ui.llm-server.your-tailnet.ts.net`
- **OpenWebUI**: `https://ai.llm-server.your-tailnet.ts.net`

---

## Why This Architecture is Correct

1. ‚úÖ **RamaLama on host**: Native Fedora package, manages model downloads
2. ‚úÖ **llama-swap in container**: Proper Podman quadlet, exposed via Caddy
3. ‚úÖ **Podman socket mounting**: Enables container-to-container management
4. ‚úÖ **llm.network**: All containers communicate via shared network
5. ‚úÖ **Vulkan GPU**: Automatic AMD acceleration via RamaLama images
6. ‚úÖ **Dynamic loading**: Models spawn on-demand
7. ‚úÖ **Fedora Atomic native**: Uses Fedora's tooling properly

This is exactly the "Fedora Atomic way" - host binaries + containerized services working together! üéØ

---

Additional Node:
the specific model/provider selection should also be configured from the chezmoi.yaml.tmpl file
meaning that when i want to add a new model to my local ramalama/llama-swap stack it will be done through this single file and it will get propagated throughout. 

minor note: 
the models defined here should also be used for embedding selection, and the task models of openwebui. meaning that ramalama is the only llm runner on my system
